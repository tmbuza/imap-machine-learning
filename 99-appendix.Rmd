# (APPENDIX) APPENDIX {-}

# Modeling Ideas

In this case study, we explore the performance of a predictive model developed to assess the likelihood of cancer in patients based on various medical records. The model incorporated patient demographics, medical history, vital signs, and diagnostic test results. When tested on a held-out dataset, it demonstrated exceptional accuracy during evaluation.

However, upon deployment to predict cancer likelihood for new patients, the model exhibited significant discrepancies in performance. This discrepancy led to an investigation to uncover potential causes for the observed inconsistency.

## Identifying the Issue

Upon closer examination, it became evident that the model's poor performance on new patients was attributed to information leakage. Specifically, one feature in the model inadvertently exposed sensitive information, which influenced the predictions. This form of information leakage compromised the model's ability to generalize to unseen data effectively.

## Implications

Information leakage poses serious implications for the predictive model's reliability and fairness. By inadvertently incorporating sensitive information, such as patient-specific details or external factors related to the prediction task, the model's performance on new data can be compromised. This can lead to inaccurate predictions and potentially harmful outcomes if relied upon in clinical decision-making.

## Recommendations

Conducting thorough feature selection and validation processes is imperative to address information leakage and ensure the model's robustness and generalizability. This involves identifying and removing features that may inadvertently expose sensitive information or introduce bias into the model. Additionally, implementing rigorous data preprocessing techniques and adhering to best practices in model development can help mitigate the risk of information leakage and enhance the reliability of predictive models.

By prioritizing transparency, fairness, and ethical considerations throughout the model development process, we can mitigate the risks associated with information leakage and build accurate and trustworthy predictive models.


## Some Effective [ML Guidelines](https://developers.google.com/machine-learning/crash-course/real-world-guidelines)
- Keep the first model simple.
- Focus on ensuring data pipeline correctness.
- Use a simple, observable metric for training & evaluation.
- Own and monitor your input features
- Treat your model configuration as code: review it, check it in
- Write down the results of all experiments, especially "failures"



```{r, include=FALSE}
knitr::opts_chunk$set(
  echo  =TRUE,
  message  =FALSE,
  warning  =FALSE,
  cache  =FALSE,
  comment  =NA,
  collapse =TRUE,
  fig.path='./figures/',
  fig.show='asis',
  dev  ='png')

```


# Using ML helper function
```{r}
load("data/ml_n_composite_object.rda", verbose = TRUE)
```

> How do you know what are the different hyperparameter options are and what are the dafault values for the different modeling approaches

```{r helper, message=FALSE, warning=FALSE}
library(mikropml)

get_hyperparams_list(ml_genus_nationality, "glmnet")
get_hyperparams_list(ml_genus_nationality, "rf")
get_hyperparams_list(ml_genus_nationality, "svmRadial")
get_hyperparams_list(ml_genus_nationality, "rpart2") # Decision tree
get_hyperparams_list(ml_genus_nationality, "xgbTree") # XGBoost

```


# IMAP GitHub Repos

```{block type="tmbinfo", echo=TRUE}

| IMAP-Repo                                      | Description                                               | GH-Pages                                             |
| :---------------------------------------------- | :--------------------------------------------------------- | :-----------------------------------------------------: |
| [OVERVIEW](https://github.com/tmbuza/imap-project-overview/) | IMAP project overview                                     | [Link](https://tmbuza.github.io/imap-project-overview/) |
| [PART 01](https://github.com/tmbuza/imap-essential-software/) | Software requirements for microbiome data analysis with Snakemake workflows | [Link](https://tmbuza.github.io/imap-essential-software/) |
| [PART 02](https://github.com/tmbuza/imap-sample-metadata/) | Downloading and exploring microbiome sample metadata from SRA Database | [Link](https://tmbuza.github.io/imap-sample-metadata/) |
| [PART 03](https://github.com/tmbuza/imap-download-sra-reads/) | Downloading and filtering microbiome sequencing data from SRA database | [Link](https://tmbuza.github.io/imap-download-sra-reads/) |
| [PART 04](https://github.com/tmbuza/imap-read-quality-control/) | Quality control of microbiome next-generation sequencing reads | [Link](https://tmbuza.github.io/imap-read-quality-control/) |
| [PART 05](https://github.com/tmbuza/imap-bioinformatics-mothur/) | Microbial profiling using MOTHUR and Snakemake workflows | [Link](https://tmbuza.github.io/imap-mothur-bioinformatics/) |
| [PART 06](https://github.com/tmbuza/imap-bioinformatics-qiime2/) | Microbial profiling using QIIME2 and Snakemake workflows | [Link](https://tmbuza.github.io/imap-qiime2-bioinformatics/) |
| [PART 07](https://github.com/tmbuza/imap-data-processing/) | Processing output from 16S-based microbiome bioinformatics pipelines | [Link](https://tmbuza.github.io/imap-data-preparation/) |
| [PART 08](https://github.com/tmbuza/imap-exploratory-analysis/) | Exploratory analysis of processed 16S-based microbiome data | [Link](https://tmbuza.github.io/imap-data-exploration/) |
| [PART 09](https://github.com/tmbuza/imap-statistical-analysis/) | Statistical analysis of processed 16S-based microbiome data | [Link](https://tmbuza.github.io/imap-statistical-analysis/) |
| [PART 10](https://github.com/tmbuza/imap-machine-learning/) | Machine learning analysis of processed 16S-based microbiome data | [Link](https://tmbuza.github.io/imap-machine-learning/) |```


# Session Information

Reproducibility relies on the ability to precisely recreate the working environment, and session information serves as a vital reference to achieve this consistency. Here we record details about the R environment, package versions, and system settings of the computing environment at the time of analysis. 

```{r sessioninfo}
library(sessioninfo)

# Get session info
info <- capture.output(print(session_info()))

# Define patterns to exclude
exclude_patterns <- c("/Users/.*", "Africa/Dar_es_Salaam") # This line is location-dependent

# Exclude lines containing specific information
info_filtered <- info[!grepl(paste(exclude_patterns, collapse = "|"), info)]

# Save the filtered session info to a text file in the root directory without line numbers
cat(info_filtered, file = "session_info.txt", sep = "\n")

```
