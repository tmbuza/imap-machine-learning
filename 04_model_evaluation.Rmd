
# Model Evaluation

Model evaluation involves assessing the performance of a machine learning model using various metrics and techniques. 

## Performance Metrics

### Confusion Matrix
A table used to describe the performance of a classification model, showing the counts of true positive, true negative, false positive, and false negative predictions.


```{r, include=FALSE}
knitr::opts_chunk$set(
  echo  =TRUE,
  message  =FALSE,
  warning  =FALSE,
  cache  =FALSE,
  comment  =NA,
  collapse =TRUE,
  fig.path='./figures/',
  fig.show='asis',
  dev  ='png')

```


```{r}
load("data/data_imputed.rda", verbose = TRUE)
load("data/train_test_data.rda", verbose = TRUE)
load("models/models.rda", verbose = TRUE)

```


```{r}

# Load necessary libraries
library(caret)
# Predict using the trained model
predictions <- predict(mod_regLogistic_cv, newdata = test_data)

# Compute confusion matrix
confusion_matrix <- caret::confusionMatrix(predictions, test_data$target)

print(confusion_matrix)

```


### Accuracy
The proportion of correctly classified instances out of the total instances.

```{r}
library(caret)
accuracy <- confusion_matrix$overall["Accuracy"]
accuracy

```


### Precision
The proportion of true positive predictions out of all positive predictions. It measures the model's ability to identify relevant instances.

```{r}
library(caret)
precision <- confusion_matrix$byClass["Precision"]

print(precision)

```


### Recall (Sensitivity)
The proportion of true positive predictions out of all actual positive instances. It measures the model's ability to capture all positive instances.


```{r}
library(caret)
recall <- confusion_matrix$byClass["Recall"]
print(recall)

```



### F1 Score
The harmonic mean of precision and recall, providing a balance between the two metrics. It is useful when the class distribution is imbalanced.


```{r}
library(caret)
f1 <- confusion_matrix$byClass["F1"]
print(f1)
```


Performance metrics dataframe
```{r}
# Create a data frame for model performance metrics
performance_metrics <- data.frame(
  Metric = c("Accuracy", "Precision", "Recall", "F1 Score"),
  Value = c(accuracy, precision, recall, f1)
)

performance_metrics

```


Visualize model performance metrics
```{r}
ggplot(performance_metrics, aes(x = Metric, y = Value)) +
  geom_bar(stat = "identity", fill = "steelblue", color = "black") +
  labs(x = "Metric", y = "Value", title = "Model Performance Metrics") +
  theme_minimal()

```


```{block, type="infoicon", echo=TRUE}
**Function for computing Specificity and Sensitivity**

library(purrr)

get_sens_spec <- function(threshold, score, actual, direction){
  
  predicted <- if(direction == "greaterthan") {
    score > threshold 
    } else {
      score < threshold
    }
  
  tp <- sum(predicted & actual)
  tn <- sum(!predicted & !actual)
  fp <- sum(predicted & !actual)
  fn <- sum(!predicted & actual)  
  
  specificity <- tn / (tn + fp)
  sensitivity <- tp / (tp + fn)
  
  tibble("specificity" = specificity, "sensitivity" = sensitivity)
}

get_roc_data <- function(x, direction){
  
  # x <- test
  # direction <- "greaterthan"
  
  thresholds <- unique(x$score) %>% sort()
  
  map_dfr(.x=thresholds, ~get_sens_spec(.x, x$score, x$srn, direction)) %>%
    rbind(c(specificity = 0, sensitivity = 1))
}

```


## ROC Curve and AUC-ROC
The ROC Curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various threshold settings. It provides a visual representation of the classifier's performance across different threshold levels. The Area Under the ROC Curve (AUC-ROC) quantifies the overall performance of the model in distinguishing between the positive and negative classes. A higher AUC-ROC value indicates better discrimination performance.


```{r}
library(pROC)
library(ggplot2)

# Compute predicted probabilities
predicted_probabilities <- predict(mod_regLogistic_cv, newdata = test_data, type = "prob")

# Extract probabilities for the positive class
positive_probabilities <- predicted_probabilities$"0"

# Compute ROC curve and AUC-ROC
roc_curve <- pROC::roc(test_data$target, positive_probabilities)
auc_roc <- pROC::auc(roc_curve)

# Plot ROC curve with descriptive axis titles
ggroc(roc_curve, color = "steelblue", size = 1) +
  labs(title = "Receiver Operating Characteristic (ROC) Curve",
       x = "False Positive Rate (1 - Specificity)",
       y = "True Positive Rate (Sensitivity)") +
  theme_minimal()


# Print AUC-ROC
cat("AUC-ROC:", auc_roc)

```


# Model Validation

## Cross-Validation Techniques

Cross-validation is a method used to evaluate the performance and generalization ability of predictive models. It involves partitioning the available data into multiple subsets, known as folds. The model is trained on a portion of the data (training set) and then evaluated on the remaining data (validation set). This process is repeated multiple times, with each fold serving as the validation set exactly once. Cross-validation provides a more robust estimate of the model's performance compared to a single train-test split and helps identify overfitting.

In this section, we will explore various cross-validation techniques. Understanding these methods is essential as they play a significant role in subsequent hyperparameter tuning. Cross-validation ensures robust model evaluation and assists in selecting optimal hyperparameters for improved model performance.

### K-Fold Cross-Validation

K-Fold Cross-Validation divides the data into K equal-sized folds. The model is trained K times, each time using K-1 folds as the training set and the remaining fold as the validation set.

```{r}
# Example of K-Fold Cross-Validation
kfcv_ctrl <- caret::trainControl(method = "cv", number = 10)

```

### Leave-One-Out Cross-Validation (LOOCV)
Leave-One-Out Cross-Validation involves using a single observation as the validation set and the remaining observations as the training set. This process is repeated for each observation in the dataset.

```{r}
# Example of Leave-One-Out Cross-Validation
loocv_ctrl <- caret::trainControl(method = "LOOCV")

```

### Stratified Cross-Validation
Stratified Cross-Validation ensures that each fold maintains the same class distribution as the original dataset. It is particularly useful for imbalanced datasets.

```{r}
# Example of Stratified Cross-Validation
strcv_ctrl <- trainControl(method = "cv", 
                     number = 10, 
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

```


### Nested Cross-Validation
Nested Cross-Validation is used to tune hyperparameters and evaluate model performance simultaneously. It involves an outer loop for model evaluation using K-Fold Cross-Validation and an inner loop for hyperparameter tuning.

```{r}
# Example of Nested Cross-Validation
nestcv_ctrl_outer <- trainControl(method = "cv", number = 5)
netscv_ctrl_inner <- trainControl(method = "cv", number = 3)

```


## Holdout Validation
Holdout validation involves splitting the dataset into two subsets: a training set used to train the model and a separate validation set used to evaluate its performance. This technique is straightforward and computationally efficient but may suffer from high variance if the validation set is small.

```{r message=FALSE, warning=FALSE}
library(caret)

# Sample dataset (replace this with your own dataset)
# data <- train_data
data <- data_imputed

# Holdout Validation
set.seed(123)  # for reproducibility

train_indices <- sample(1:nrow(data), 0.8 * nrow(data))  # 80% of data for training
train_data <- data[train_indices, ]
validation_data <- data[-train_indices, ]

# Define your model
# model <- train(target ~ ., data = train_data, method = "glm", family = binomial)
model <- train(target ~ ., data = train_data, method = "glm", family = binomial)

# Make predictions on validation data
predictions <- predict(model, newdata = validation_data)

# Evaluate model performance
# evaluation_metrics <- confusionMatrix(predictions, validation_data$target)
evaluation_metrics <- confusionMatrix(predictions, validation_data$target)
print(evaluation_metrics)

```


## Bootstrapping
Bootstrapping is a resampling technique where multiple datasets are generated by randomly sampling with replacement from the original dataset. Each dataset is used to train a separate model, and their aggregate predictions are used to assess the model's performance. Bootstrapping provides robust estimates of model performance and can handle small datasets effectively.

```{r message=FALSE, warning=FALSE}
library(caret)

# Sample dataset (replace this with your own dataset)
data <- data_imputed

# Define your model
model <- train(target ~ ., data = data, method = "glm", family = binomial)

# Perform bootstrapping
set.seed(123)  # for reproducibility
boot <- createResample(y = data$target, times = 5)
boot_results <- lapply(boot, function(index) {
  train_data <- data[index, ]
  model <- train(target ~ ., data = train_data, method = "glm", family = binomial)
  predict(model, newdata = data[-index, ])
})

# Aggregate bootstrapped results
boot_predictions <- do.call(c, boot_results)

# Ensure boot_predictions and data$target have the same length
min_length <- min(length(boot_predictions), length(data$target))
boot_predictions <- boot_predictions[1:min_length]
data$target <- data$target[1:min_length]

# Evaluate bootstrapped model performance
evaluation_metrics <- confusionMatrix(boot_predictions, data$target)
print(evaluation_metrics)

```


