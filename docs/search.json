[{"path":"index.html","id":"machine-learning","chapter":"IMAP-Machine Learning: Predictive Modeling Using Microbiome Data","heading":"IMAP-Machine Learning: Predictive Modeling Using Microbiome Data","text":"","code":""},{"path":"index.html","id":"welcome-to-machine-learning-for-microbiome-data","chapter":"IMAP-Machine Learning: Predictive Modeling Using Microbiome Data","heading":"Welcome to Machine Learning for Microbiome Data","text":"Welcome exploration machine learning’s pivotal role deciphering microbiome data. realm biomedical research, understanding microbial communities interactions host organisms crucial unraveling complexities health disease.Machine learning techniques offer powerful tools analyze vast amounts genetic, metagenomic, metadata generated microbiome studies. leveraging computational algorithms, researchers can extract meaningful patterns, predict microbial behavior, uncover associations various health conditions.Throughout guide, ’ll delve application machine learning microbiome research, exploring computational techniques revolutionizing understanding microbial ecosystems. Together, ’ll uncover insights intricate relationships microbial communities implications human health environmental sustainability.Join us journey navigate fascinating intersection machine learning microbiome research, discover transformative potential computational techniques advancing biomedical science.","code":""},{"path":"key-components-of-microbiome-machine-learning.html","id":"key-components-of-microbiome-machine-learning","chapter":"1 Key Components of Microbiome Machine Learning","heading":"1 Key Components of Microbiome Machine Learning","text":"Machine learning microbiome research encompasses several key components, playing vital role extracting insights complex microbial data:Feature Engineering: core microbiome analysis lies extraction relevant features diverse data sources, including microbial abundance, diversity metrics, host metadata. Effective feature engineering lays groundwork subsequent analysis model development.Feature Engineering: core microbiome analysis lies extraction relevant features diverse data sources, including microbial abundance, diversity metrics, host metadata. Effective feature engineering lays groundwork subsequent analysis model development.Classification Prediction: Machine learning models play crucial role classifying samples distinct groups (e.g., healthy vs. diseased) predicting clinical outcomes based microbiome profiles. Algorithms Random Forests Neural Networks enable accurate predictions, aiding disease diagnosis prognosis.Classification Prediction: Machine learning models play crucial role classifying samples distinct groups (e.g., healthy vs. diseased) predicting clinical outcomes based microbiome profiles. Algorithms Random Forests Neural Networks enable accurate predictions, aiding disease diagnosis prognosis.Community Profiling: Unsupervised learning techniques, clustering ordination, unveil microbial community structures similarities. characterizing microbial ecosystems, community profiling enhances understanding microbial diversity ecological dynamics.Community Profiling: Unsupervised learning techniques, clustering ordination, unveil microbial community structures similarities. characterizing microbial ecosystems, community profiling enhances understanding microbial diversity ecological dynamics.Microbial Interaction Networks: Graph-based methods uncover intricate co-occurrence patterns identify keystone species within microbial communities. Understanding microbial interactions provides insights ecosystem stability, resilience, functional diversity.Microbial Interaction Networks: Graph-based methods uncover intricate co-occurrence patterns identify keystone species within microbial communities. Understanding microbial interactions provides insights ecosystem stability, resilience, functional diversity.Ecological Modeling: Machine learning algorithms model microbial ecosystem dynamics, capturing complex interactions microbial taxa environment. Ecological modeling elucidates succession patterns, responses environmental changes, ecosystem-level processes.Ecological Modeling: Machine learning algorithms model microbial ecosystem dynamics, capturing complex interactions microbial taxa environment. Ecological modeling elucidates succession patterns, responses environmental changes, ecosystem-level processes.Biomarker Discovery: Feature selection techniques identify microbial taxa pathways serving biomarkers specific health conditions environmental factors. Biomarker discovery facilitates early disease detection, personalized treatment strategies, environmental monitoring.Biomarker Discovery: Feature selection techniques identify microbial taxa pathways serving biomarkers specific health conditions environmental factors. Biomarker discovery facilitates early disease detection, personalized treatment strategies, environmental monitoring.Personalized Medicine:** Machine learning enables development personalized interventions based individual microbiome profiles. integrating microbiome data clinical information, personalized medicine offers targeted therapies preventive strategies tailored individual’s unique microbial composition.Personalized Medicine:** Machine learning enables development personalized interventions based individual microbiome profiles. integrating microbiome data clinical information, personalized medicine offers targeted therapies preventive strategies tailored individual’s unique microbial composition.Together, key components form foundation microbiome machine learning, driving advancements biomedical research, personalized healthcare, environmental sustainability.","code":""},{"path":"ml-framework_R.html","id":"ml-framework_R","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","text":"comprehensive framework outlining key steps involved leveraging machine learning techniques analyzing microbiome data R.","code":""},{"path":"ml-framework_R.html","id":"data-acquisition","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.1 Data Acquisition","text":"NCBI project: PRJEB13870Title: Gut microbiota dysbiosis contributes development hypertension (Zhao et al., 2017)","code":""},{"path":"ml-framework_R.html","id":"study-description","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.2 Study Description","text":"Metagenomic MetabonomicsFecal microbiota transplantationIdentified dysbiosis gut microbiome contributor hypertension pathogenesis metabolic effects alteration","code":""},{"path":"ml-framework_R.html","id":"data-cleaning-and-tidying","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.3 Data Cleaning and Tidying","text":"Feature OTU tableTaxonomy tableMetadataMetabolic pathwaysOther experimental data…","code":""},{"path":"ml-framework_R.html","id":"exploratory-data-analysis","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.4 Exploratory Data Analysis","text":"Diversity analysisTaxonomic profilingDifferential abundance analysisFunctional profiling","code":""},{"path":"ml-framework_R.html","id":"feature-engineering","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.5 Feature Engineering","text":"Dimensionality reduction techniques (e.g., PCA, t-SNE)Feature selection methods (e.g., Boruta, LASSO)","code":""},{"path":"ml-framework_R.html","id":"model-development","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.6 Model Development","text":"Selection appropriate machine learning algorithms (e.g., Random Forest, Support Vector Machines)Hyperparameter tuning using cross-validationModel evaluation metrics (e.g., accuracy, precision, recall, F1-score)","code":""},{"path":"ml-framework_R.html","id":"model-interpretation","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.7 Model Interpretation","text":"Feature importance analysisVisualization model predictions (e.g., ROC curves, confusion matrices)","code":""},{"path":"ml-framework_R.html","id":"integration-with-biological-knowledge","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.8 Integration with Biological Knowledge","text":"Interpretation model results context biological mechanismsIdentification potential biomarkers therapeutic targets","code":""},{"path":"ml-framework_R.html","id":"deployment-and-validation","chapter":"2 Machine Learning Framework in R: From Data Acquisition to Model Deployment","heading":"2.9 Deployment and Validation","text":"Application trained models new datasetsValidation model performance independent cohorts","code":""},{"path":"case-study-applied-machine-learning-for-microbiome-data-analysis.html","id":"case-study-applied-machine-learning-for-microbiome-data-analysis","chapter":"3 Case Study: Applied Machine Learning for Microbiome Data Analysis","heading":"3 Case Study: Applied Machine Learning for Microbiome Data Analysis","text":"","code":""},{"path":"case-study-applied-machine-learning-for-microbiome-data-analysis.html","id":"data-acquisition-1","chapter":"3 Case Study: Applied Machine Learning for Microbiome Data Analysis","heading":"3.1 Data Acquisition","text":"data practical guide obtained NCBI (National Center Biotechnology Information) project accession number PRJEB13870. project titled “Gut microbiota dysbiosis contributes development hypertension” serves ideal resource demonstrate process building predictive analysis model. dataset offers valuable insights association gut microbiota composition hypertension development, critical area research within fields microbiology cardiovascular health.","code":""},{"path":"case-study-applied-machine-learning-for-microbiome-data-analysis.html","id":"study-description-1","chapter":"3 Case Study: Applied Machine Learning for Microbiome Data Analysis","heading":"3.2 Study description","text":"study employed multifaceted approach, integrating metagenomics metabonomics analyses alongside fecal microbiota transplantation (FMT). investigating dysbiosis gut microbiome, study elucidated role contributing factor pathogenesis hypertension, primarily alterations metabolic effects. methodologies, research aimed provide comprehensive understanding intricate relationship gut microbiota composition development hypertension.","code":""},{"path":"case-study-applied-machine-learning-for-microbiome-data-analysis.html","id":"data-integration","chapter":"3 Case Study: Applied Machine Learning for Microbiome Data Analysis","heading":"3.3 Data integration","text":"section outlines steps involved processing OTU table, taxonomy data, metabolites, metadata, aligning multifaceted approach employed study description.","code":"\n# Load necessary libraries with suppressed startup messages\nlibrary(tidyverse, suppressPackageStartupMessages())\nlibrary(broom)\nlibrary(ggtext)\nlibrary(data.table)\n\n# Set seed for reproducibility\nset.seed(2022)\n\n# Read and process the OTU table data\notutable <- read_csv(\"data/HypertensionProject.csv\", show_col_types = FALSE) %>%\n  dplyr::select(1, Prevotella:ncol(.)) %>%\n  data.table::transpose(keep.names = \"taxonomy\", make.names = \"SampleID\") %>%\n  pivot_longer(-taxonomy, names_to=\"sample_id\", values_to=\"rel_abund\") %>%\n  relocate(sample_id)\n\n# Read and process the metabolites data\nmetabolites <- read_csv(\"data/HypertensionProjectMetabolites.csv\", show_col_types = FALSE) %>%\n  dplyr::select(c(1,5:18)) %>%\n  data.table::transpose(keep.names = \"metabopwy\", make.names = \"SampleID\") %>%\n  pivot_longer(-metabopwy, names_to=\"sample_id\", values_to=\"value\") %>%\n  group_by(sample_id) %>% \n  mutate(rel_abund = value/sum(value)) %>% \n  ungroup() %>% \n  dplyr::select(-value) %>% \n  relocate(sample_id)\n\n# Read and process the taxonomy data\ntaxonomy <- read_tsv(\"data/mo_demodata/baxter.cons.taxonomy\", show_col_types = FALSE) %>%\n  rename_all(tolower) %>%\n  dplyr::select(otu, taxonomy) %>%\n  mutate(taxonomy = str_replace_all(taxonomy, \"\\\\(\\\\d+\\\\)\", \"\"),\n         taxonomy = str_replace(taxonomy, \";unclassified\", \"_unclassified\"),\n         taxonomy = str_replace_all(taxonomy, \";unclassified\", \"\"),\n         taxonomy = str_replace_all(taxonomy, \";$\", \"\"),\n         taxonomy = str_replace_all(taxonomy, \".*;\", \"\"))\n\n# Read and process the metadata\nmetadata <- read_csv(\"data/HypertensionProject.csv\", show_col_types = FALSE) %>%\n  dplyr::select(c(1:3)) %>%\n  mutate(hyper = Disease_State == \"HTN\" | Disease_State == \"pHTN\",\n         control = Disease_State == \"healthy\") %>%\n  rename(sample_id = SampleID)\n\n## Data joining\n\n# Join metadata with OTU table to create composite dataset\ncomposite <- inner_join(metadata, otutable, by=\"sample_id\")\n\n# Join metadata with metabolites data to create composite metabolites dataset\nmetabo_composite <- inner_join(metadata, metabolites, by=\"sample_id\")"},{"path":"data-for-machine-learning-analysis.html","id":"data-for-machine-learning-analysis","chapter":"4 Data for Machine Learning Analysis","heading":"4 Data for Machine Learning Analysis","text":"section outlines preprocessing steps involved preparing data subsets tailored machine learning analysis. code segments transform raw data structured formats suitable predictive modeling. Specifically, subsets created encompass various combinations taxonomic metabolic features alongside binary labels representing selected features. subset undergoes specific preprocessing steps, including data selection, transformation, encoding, ensure compatibility machine learning algorithms.\nNote users:\n\nencounter issues pivot_wider() returns values \n<list>, may due multiple values combination\nidentifiers (e.g., sample ID feature). cases, consider\naggregating values using group_by() summarise() \npivoting.\n\nExample:\n\nps_df %>% select(sample_id, taxon, nationality, rel_abund, bmi)\n%>% mutate(taxon = str_replace_all(taxon, “*“,”“)) %>%\ngroup_by(sample_id, taxon, nationality, bmi) %>% summarise(rel_abund\n= mean(rel_abund), .groups =”drop”) %>% pivot_wider(names_from =\ntaxon, values_from = rel_abund) %>% mutate(nationality =\nif_else(nationality == “AAM”, “0”, “1”)) %>% mutate(bmi = if_else(bmi\n== “overweight” | bmi == “obese” , “0”, “1”)) %>%\nselect(-c(sample_id, bmi))\n\nensures rel_abund values properly aggregated \npivoting.\n","code":"\n# Subset for machine learning analysis: Taxonomic genus features with disease states\n\nml_genus_dsestate <- composite %>%\n  select(sample_id, taxonomy, enttype = Enterotype, rel_abund, dsestate = Disease_State) %>%\n  pivot_wider(names_from=taxonomy, values_from = rel_abund) %>%\n  select(-sample_id) %>%\n  mutate(enttype = if_else(enttype == \"Enterotype_1\", \"0\", \"1\")) %>%\n  mutate(dsestate = if_else(dsestate == \"pHTN\" | dsestate == \"HTN\" , \"0\", \"1\")) %>%\n  select(-enttype) %>%\n  select(dsestate, everything())\n\n# Subset for machine learning analysis: Taxonomic genus features with enterotypes\nml_genus_enttype <- composite %>%\n  select(sample_id, taxonomy, enttype = Enterotype, rel_abund, dsestate = Disease_State) %>%\n  pivot_wider(names_from=taxonomy, values_from = rel_abund) %>%\n  select(-sample_id) %>%\n  mutate(enttype = if_else(enttype == \"Enterotype_1\", \"0\", \"1\")) %>%\n  mutate(dsestate = if_else(dsestate == \"pHTN\" | dsestate == \"HTN\" , \"0\", \"1\")) %>%\n  select(-dsestate) %>%\n  select(enttype, everything())\n\n# Using dietswap dataset from microbiome package\n\nload(\"../imap-data-preparation/data/phyloseq_raw_rel_psextra_df_objects.rda\")\nlibrary(dplyr)\nlibrary(tidyr)\n\n\n# Nationality feature\nml_genus_nationality <- ps_df %>%\n  select(sample_id, taxon, nationality, rel_abund, bmi) %>%\n  mutate(\n    taxon = str_replace_all(taxon, \"\\\\*\", \"\"),\n    nationality = factor(if_else(nationality == \"AAM\", \"0\", \"1\"), levels = c(\"0\", \"1\")),\n    bmi = factor(if_else(bmi == \"overweight\" | bmi == \"obese\", \"0\", \"1\"), levels = c(\"0\", \"1\"))\n  ) %>%\n  group_by(sample_id, taxon, nationality, bmi) %>%\n  summarise(rel_abund = mean(rel_abund), .groups = \"drop\") %>%\n  pivot_wider(names_from = taxon, values_from = rel_abund) %>%\n  ungroup() %>%\n  filter(!is.na(nationality)) %>%  # Remove rows with NA in the 'nationality' column\n  select(-c(sample_id, bmi)) %>%\n  mutate(across(starts_with(\"rel_abund\"), as.numeric))\n\n# Body mass index feature\nml_genus_bmi <- ps_df %>%\n  select(sample_id, taxon, nationality, rel_abund, bmi) %>%\n  mutate(\n    taxon = str_replace_all(taxon, \"\\\\*\", \"\"),\n    nationality = factor(if_else(nationality == \"AAM\", \"0\", \"1\"), levels = c(\"0\", \"1\")),\n    bmi = factor(if_else(bmi == \"overweight\" | bmi == \"obese\", \"0\", \"1\"), levels = c(\"0\", \"1\"))\n  ) %>%\n  group_by(sample_id, taxon, nationality, bmi) %>%\n  summarise(rel_abund = mean(rel_abund), .groups = \"drop\") %>%\n  pivot_wider(names_from = taxon, values_from = rel_abund) %>%\n  ungroup() %>%\n  filter(!is.na(bmi)) %>%  # Remove rows with NA in the 'bmi' column\n  select(-c(sample_id, nationality)) %>%\n  mutate(across(starts_with(\"rel_abund\"), as.numeric))\n\n# Save the processed data objects into an RDA file\nsave(otutable, \n     metabolites, \n     taxonomy, \n     metadata, \n     composite, \n     metabo_composite, \n     ml_genus_nationality, \n     ml_genus_enttype, \n     ml_genus_nationality, \n     ml_genus_bmi, \n     file = \"data/ml_n_composite_object.rda\")"},{"path":"specificity-and-sensitivity-function.html","id":"specificity-and-sensitivity-function","chapter":"5 Specificity and Sensitivity function","heading":"5 Specificity and Sensitivity function","text":"","code":"\nlibrary(purrr)\n\nget_sens_spec <- function(threshold, score, actual, direction){\n  \n  predicted <- if(direction == \"greaterthan\") {\n    score > threshold \n    } else {\n      score < threshold\n    }\n  \n  tp <- sum(predicted & actual)\n  tn <- sum(!predicted & !actual)\n  fp <- sum(predicted & !actual)\n  fn <- sum(!predicted & actual)  \n  \n  specificity <- tn / (tn + fp)\n  sensitivity <- tp / (tp + fn)\n  \n  tibble(\"specificity\" = specificity, \"sensitivity\" = sensitivity)\n}\n\nget_roc_data <- function(x, direction){\n  \n  # x <- test\n  # direction <- \"greaterthan\"\n  \n  thresholds <- unique(x$score) %>% sort()\n  \n  map_dfr(.x=thresholds, ~get_sens_spec(.x, x$score, x$srn, direction)) %>%\n    rbind(c(specificity = 0, sensitivity = 1))\n}"},{"path":"specificity-and-sensitivity-function.html","id":"significant-genera-with-wilcox.test","chapter":"5 Specificity and Sensitivity function","heading":"5.1 Significant genera with wilcox.test","text":"","code":"\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tidyr)\n\nall_genera <- composite %>%\n  tidyr::nest(data = -taxonomy) %>%\n  mutate(test = purrr::map(.x=data, ~wilcox.test(rel_abund~hyper, data=.x) %>% tidy)) %>%\n  tidyr::unnest(test) %>%\n  mutate(p.adjust = p.adjust(p.value, method=\"BH\"))\n\nsig_genera <- all_genera %>% \n  dplyr::filter(p.value < 0.05) %>%\n  arrange(p.adjust) %>% \n  dplyr::select(taxonomy, p.value)"},{"path":"specificity-and-sensitivity-function.html","id":"view-distribution-of-significant-genera","chapter":"5 Specificity and Sensitivity function","heading":"5.2 View distribution of significant genera","text":"","code":"\ncomposite %>%\n  inner_join(sig_genera, by=\"taxonomy\") %>%\n  mutate(rel_abund = 100 * (rel_abund + 1/20000),\n         taxonomy = str_replace(taxonomy, \"(.*)\", \"*\\\\1*\"),\n         taxonomy = str_replace(taxonomy, \"\\\\*(.*)_unclassified\\\\*\",\n                                \"Unclassified<br>*\\\\1*\"),\n         hyper = factor(hyper, levels = c(T, F))) %>%\n  ggplot(aes(x=rel_abund, y=taxonomy, color=hyper, fill=hyper)) +\n  # geom_vline(xintercept = 100/10530, size=0.5, color=\"gray\") +\n  geom_jitter(position = position_jitterdodge(dodge.width = 0.8,\n                                              jitter.width = 0.5),\n              shape=21) +\n  stat_summary(fun.data = median_hilow, fun.args = list(conf.int=0.5),\n               geom=\"pointrange\",\n               position = position_dodge(width=0.8),\n               color=\"black\", show.legend = FALSE) +\n  scale_x_log10() +\n  scale_color_manual(NULL,\n                     breaks = c(F, T),\n                     values = c(\"gray\", \"dodgerblue\"),\n                     labels = c(\"Control\", \"Hypertension\")) +\n  scale_fill_manual(NULL,\n                     breaks = c(F, T),\n                     values = c(\"gray\", \"dodgerblue\"),\n                     labels = c(\"Control\", \"Hypertension\")) +\n  labs(x= \"Relative abundance (%)\", y=NULL) +\n  theme_classic() +\n  theme(\n    axis.text.y = element_markdown()\n  )\nggsave(\"figures/significant_genera.tiff\", width=6, height=4)"},{"path":"specificity-and-sensitivity-function.html","id":"significant-pathways","chapter":"5 Specificity and Sensitivity function","heading":"5.3 Significant pathways","text":"Compute significant pathways using wilcox.test.","code":"\nlibrary(tidyverse)\n\nall_metabopwy <- metabo_composite %>%\n  tidyr::nest(data = -metabopwy) %>%\n  mutate(test = purrr::map(.x=data, ~wilcox.test(rel_abund~hyper, data=.x) %>% tidy)) %>%\n  tidyr::unnest(test) %>%\n  mutate(p.adjust = p.adjust(p.value, method=\"BH\"))\n\nsig_metabopwy <- all_metabopwy %>% \n  dplyr::filter(p.value < 0.3) %>% # Typically, the best significant p-value is set at 0.05\n  dplyr::select(metabopwy, p.value)"},{"path":"specificity-and-sensitivity-function.html","id":"view-distribution-of-significant-metabolic-pathways","chapter":"5 Specificity and Sensitivity function","heading":"5.4 View distribution of significant metabolic pathways","text":"Compute significant pathways, thenP-values Adjusted P-values (p.adjust) can used measure significance levels.View distribution significant pathways.filter metabolic pathways lesser stringent p.values (p < 0.25) demo purposes.","code":"\nmetabo_composite %>%\n  inner_join(sig_metabopwy, by=\"metabopwy\") %>%\n  mutate(rel_abund = 100 * (rel_abund + 1/20000),\n         metabopwy = str_replace(metabopwy, \"(.*)\", \"*\\\\1*\"),\n         metabopwy = str_replace(metabopwy, \"\\\\*(.*)_unclassified\\\\*\",\n                                \"Unclassified<br>*\\\\1*\"),\n         hyper = factor(hyper, levels = c(T, F))) %>%\n  ggplot(aes(x=rel_abund, y=metabopwy, color=hyper, fill=hyper)) +\n  geom_jitter(position = position_jitterdodge(dodge.width = 0.8,\n                                              jitter.width = 0.5),\n              shape=21) +\n  stat_summary(fun.data = median_hilow, fun.args = list(conf.int=0.5),\n               geom=\"pointrange\",\n               position = position_dodge(width=0.8),\n               color=\"black\", show.legend = FALSE) +\n  scale_x_log10() +\n  scale_color_manual(NULL,\n                     breaks = c(F, T),\n                     values = c(\"gray\", \"dodgerblue\"),\n                     labels = c(\"Control\", \"Hypertension\")) +\n  scale_fill_manual(NULL,\n                     breaks = c(F, T),\n                     values = c(\"gray\", \"dodgerblue\"),\n                     labels = c(\"Control\", \"Hypertension\")) +\n  labs(x= \"Relative abundance (%)\", y=NULL) +\n  theme_classic() +\n  theme(\n    axis.text.y = element_markdown()\n  )\nggsave(\"figures/significant_genera.tiff\", width=6, height=4)"},{"path":"specificity-and-sensitivity-function.html","id":"roc-curve-receiver-operating-characteristic-curve","chapter":"5 Specificity and Sensitivity function","heading":"5.5 ROC curve: Receiver Operating Characteristic curve","text":"Shows performance classification model classification thresholds.Plots True Positive Rate (TPR = Sensitivity) False Positive Rate (FPR = 1 - Specificity) classification thresholds.","code":""},{"path":"specificity-and-sensitivity-function.html","id":"auc-area-under-the-roc-curve","chapter":"5 Specificity and Sensitivity function","heading":"5.6 AUC: Area Under the ROC Curve","text":"measures entire two-dimensional area underneath entire ROC curve.calculus can represented (0,0) (1,1).AUC provides aggregate measure performance across possible classification thresholds.AUC desirable following two reasons:AUC scale-invariant. measures well predictions ranked, rather absolute values.AUC classification-threshold-invariant. measures quality model’s predictions irrespective classification threshold chosen[]","code":""},{"path":"specificity-and-sensitivity-function.html","id":"load-functions-and-data-objects","chapter":"5 Specificity and Sensitivity function","heading":"5.7 Load functions and data objects","text":"Functions computing:SensitivitySpecificityROCData objects ROC curve\n- composite\n- metabo_composite","code":"\nget_sens_spec <- function(threshold, score, actual, direction){\n\n  predicted <- if(direction == \"greaterthan\") {\n    score > threshold\n  } else {\n    score < threshold\n  }\n\n  tp <- sum(predicted & actual)\n  tn <- sum(!predicted & !actual)\n  fp <- sum(predicted & !actual)\n  fn <- sum(!predicted & actual)\n\n  specificity <- tn / (tn + fp)\n  sensitivity <- tp / (tp + fn)\n\n  tibble(\"specificity\" = specificity, \"sensitivity\" = sensitivity)\n}\n\nget_roc_data <- function(x, direction){\n  thresholds <- unique(x$score) %>% sort()\n\n  map_dfr(.x=thresholds, ~get_sens_spec(.x, x$score, x$hyper, direction)) %>%\n    rbind(c(specificity = 0, sensitivity = 1))\n\n}\nload(\"data/composite_object.rda\", verbose = T)## Loading objects:\n##   otutable\n##   metabolites\n##   taxonomy\n##   metadata\n##   composite\n##   metabo_composite"},{"path":"specificity-and-sensitivity-function.html","id":"prepare-roc-data","chapter":"5 Specificity and Sensitivity function","heading":"5.8 Prepare ROC data","text":"Note: separate lines biomarker feature shown legend!","code":"\nlibrary(purrr)\nlibrary(dplyr)\nlibrary(tidyr) # for nest() & unnest()\n\nroc_data <- composite %>%\n  rename_all(tolower) %>% \n  inner_join(sig_genera, by=\"taxonomy\") %>%\n  dplyr::select(sample_id, taxonomy, rel_abund, hyper) %>%\n  pivot_wider(names_from=taxonomy, values_from=rel_abund) %>%\n  pivot_longer(cols=-c(sample_id, hyper), names_to=\"feature\", values_to=\"score\") %>%\n  nest(data = -feature) %>%\n  mutate(direction = if_else(feature == \"Ruminiclostridium\", \"lessthan\",\"greaterthan\")) %>%\n  mutate(roc_data = map2(.x = data, .y=direction, ~ get_roc_data(.x, .y))) %>%\n  unnest(roc_data) %>%\n  dplyr::select(feature, specificity, sensitivity)\n\n## Plot ROC data\nroc_data %>%\n  ggplot(aes(x=1-specificity, y=sensitivity, color = feature)) +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, color=\"gray\") +\n  theme_classic() + \n  labs(x = \"False Positive Rate\", y = \"True Positive Rate\", color = \"Feature\")\nggsave(\"figures/roc_figure.tiff\", width=6, height=4)"},{"path":"specificity-and-sensitivity-function.html","id":"metabolic-pathway-data","chapter":"5 Specificity and Sensitivity function","heading":"5.9 Metabolic pathway data","text":"Note: separate lines biomarker feature shown legend!\ndata can added get better improved classification.","code":"\nlibrary(dplyr)\nlibrary(tidyr) # for nest() & unnest()\n\nmetabo_roc_data <- metabo_composite %>%\n  rename_all(tolower) %>% \n  inner_join(sig_metabopwy, by=\"metabopwy\") %>%\n  dplyr::select(sample_id, metabopwy, rel_abund, hyper) %>%\n  pivot_wider(names_from=metabopwy, values_from=rel_abund) %>%\n  pivot_longer(cols=-c(sample_id, hyper), names_to=\"feature\", values_to=\"score\") %>%\n  nest(data = -feature) %>%\n  mutate(direction = if_else(feature == \"BCAA_transport\", \"lessthan\",\"greaterthan\")) %>%\n  mutate(roc_data = map2(.x = data, .y=direction, ~ get_roc_data(.x, .y))) %>%\n  unnest(roc_data) %>%\n  dplyr::select(feature, specificity, sensitivity)\n\n\nmetabo_roc_data %>%\n  ggplot(aes(x=1-specificity, y=sensitivity, color = feature)) +\n  geom_line() +\n  geom_abline(slope = 1, intercept = 0, color=\"gray\") +\n  theme_classic() + \n  labs(x = \"False Positive Rate\", y = \"True Positive Rate\", color = \"Feature\")\nggsave(\"figures/roc_figure.tiff\", width=6, height=4)"},{"path":"case-study-predictive-model-performance-affected-by-information-leakage.html","id":"case-study-predictive-model-performance-affected-by-information-leakage","chapter":"6 Case Study: Predictive Model Performance Affected by Information Leakage","heading":"6 Case Study: Predictive Model Performance Affected by Information Leakage","text":"case study, explore performance predictive model developed assess likelihood cancer patients based various medical records. model incorporated patient demographics, medical history, vital signs, diagnostic test results. tested held-dataset, demonstrated exceptional accuracy evaluation.However, upon deployment predict cancer likelihood new patients, model exhibited significant discrepancies performance. discrepancy led investigation uncover potential causes observed inconsistency.Identifying Issue:Upon closer examination, became evident model’s poor performance new patients attributed information leakage. Specifically, one feature model inadvertently exposed sensitive information, influenced predictions. form information leakage compromised model’s ability generalize unseen data effectively.Implications:Information leakage poses serious implications predictive model’s reliability fairness. inadvertently incorporating sensitive information, patient-specific details external factors related prediction task, model’s performance new data can compromised. can lead inaccurate predictions potentially harmful outcomes relied upon clinical decision-making.Recommendations:Conducting thorough feature selection validation processes imperative address information leakage ensure model’s robustness generalizability. involves identifying removing features may inadvertently expose sensitive information introduce bias model. Additionally, implementing rigorous data preprocessing techniques adhering best practices model development can help mitigate risk information leakage enhance reliability predictive models.prioritizing transparency, fairness, ethical considerations throughout model development process, can mitigate risks associated information leakage build accurate trustworthy predictive models.","code":""},{"path":"case-study-predictive-model-performance-affected-by-information-leakage.html","id":"some-effective-ml-guidelines","chapter":"6 Case Study: Predictive Model Performance Affected by Information Leakage","heading":"6.1 Some Effective ML Guidelines","text":"Keep first model simple.Focus ensuring data pipeline correctness.Use simple, observable metric training & evaluation.monitor input featuresTreat model configuration code: review , check inWrite results experiments, especially “failures”","code":"\n\n<!--chapter:end:02_case_study.Rmd-->\n\n# (PART) LOGISTIC REGRESSION IN R {-}\n\n# Microbiome Logistic Regression\n\nIn this section, we will delve into the application of logistic regression models for analyzing microbiome data. Logistic regression is a powerful statistical method used for binary classification tasks, making it particularly useful for predicting categorical outcomes based on microbiome profiles.\n\n\n\n```r\nknitr::opts_chunk$set(\n  echo  =TRUE,\n  message  =FALSE,\n  warning  =FALSE,\n  cache  =FALSE,\n  fig.path = \"figures\",\n  comment  =NA)"},{"path":"case-study-predictive-model-performance-affected-by-information-leakage.html","id":"importing-data-and-libraries","chapter":"6 Case Study: Predictive Model Performance Affected by Information Leakage","heading":"6.2 Importing Data and Libraries","text":"begin, let’s import necessary data libraries required analysis.","code":"\nsource(\"R/genus_pwy_process.R\")\nlibrary(tidyverse)\nlibrary(parallelly)\nlibrary(future)\nlibrary(mikropml)\nlibrary(caret)\nlibrary(tictoc)\nlibrary(furrr)\nlibrary(cgwtools)"},{"path":"case-study-predictive-model-performance-affected-by-information-leakage.html","id":"checking-available-cores","chapter":"6 Case Study: Predictive Model Performance Affected by Information Leakage","heading":"6.3 Checking Available Cores","text":"’s essential check available processing cores, may influence computational load can handled efficiently.","code":"\navailableCores()system \n     8 "},{"path":"common-packages-for-machine-learning-in-r.html","id":"common-packages-for-machine-learning-in-r","chapter":"7 Common Packages for Machine Learning in R","heading":"7 Common Packages for Machine Learning in R","text":"","code":""},{"path":"common-packages-for-machine-learning-in-r.html","id":"mikropml","chapter":"7 Common Packages for Machine Learning in R","heading":"7.1 Mikropml","text":"Mikropml versatile R package tailored specifically microbiome data analysis machine learning tasks. offers comprehensive suite tools preprocessing microbiome data, constructing predictive models, assessing performance. integrating popular machine learning methods like glmnet caret, Mikropml provides seamless workflow analyzing microbiome datasets building predictive models customized address specific research inquiries. guide, utilize Mikropml, integrates glmnet caret, enhancing efficiency effectiveness machine learning analyses.","code":""},{"path":"common-packages-for-machine-learning-in-r.html","id":"glmnet","chapter":"7 Common Packages for Machine Learning in R","heading":"7.2 Glmnet","text":"Glmnet stands cornerstone package R’s machine learning landscape, primarily utilized fitting generalized linear models penalized maximum likelihood. Offering efficient algorithms implementing regularized regression models, including ridge lasso regression, Glmnet invaluable analyzing high-dimensional data. functionalities extend prediction, model visualization, cross-validation, rendering vital asset constructing robust interpretable models.","code":""},{"path":"common-packages-for-machine-learning-in-r.html","id":"caret","chapter":"7 Common Packages for Machine Learning in R","heading":"7.3 Caret","text":"Caret, short Classification REgression Training, emerges comprehensive package within R’s machine learning arsenal. Encompassing broad spectrum algorithms techniques spanning classification, regression, feature selection, Caret provides unified framework model training evaluation. user-friendly interface facilitates comparison different algorithms enables efficient parameter tuning. capabilities ranging data preprocessing model visualization, Caret serves indispensable tool practitioners navigating complexities machine learning R.","code":""},{"path":"common-packages-for-machine-learning-in-r.html","id":"preprocessing-and-transformation","chapter":"7 Common Packages for Machine Learning in R","heading":"7.4 Preprocessing and transformation","text":"","code":"\nlibrary( mikropml)\n\nsource(\"R/genus_pwy_process.R\")\nhyper_genus_data <- composite %>%\n  select(sample_id, taxonomy, rel_abund, hyper) %>%\n  pivot_wider(names_from=taxonomy, values_from = rel_abund) %>%\n  select(-sample_id) %>%\n  mutate(hyper = if_else(hyper, \"hyper\", \"healthy\")) %>%\n  select(hyper, everything())\n\nhyper_genus_preprocess <- preprocess_data(hyper_genus_data,\n                                        outcome_colname = \"hyper\")$dat_transformed"},{"path":"common-packages-for-machine-learning-in-r.html","id":"tuning-parameters-with-multiple-alpha","chapter":"7 Common Packages for Machine Learning in R","heading":"7.5 Tuning parameters with multiple alpha","text":"","code":"\ntest_hp <- list(alpha = c(0, 0.5, 1), \n                # lambda = c(0.1, 1, 2, 3, 4, 5, 10))\n                lambda = c(0.1, 1, 2))"},{"path":"common-packages-for-machine-learning-in-r.html","id":"function-for-model-training","chapter":"7 Common Packages for Machine Learning in R","heading":"7.6 Function for model training","text":"Useful terminologiesCross-validationUsed machine learning improving model predictionUseful don’t enough efficient methods like 3-way split (train, validation test) holdout dataset.K-Fold\nvalidation technique\nData split k-subsets\nHoldout method repeated k-times k subsets used test set k-1 subsets used training purpose.\nvalidation techniqueData split k-subsetsHoldout method repeated k-times k subsets used test set k-1 subsets used training purpose.","code":"\nget_hp_results <- function(seed){\n  \n  run_ml(hyper_genus_preprocess,\n       method=\"glmnet\",\n       outcome_colname = \"hyper\",\n       kfold = 5,\n       cv_times = 100,\n       training_frac = 0.8, # training set: 80% \n       hyperparameters = test_hp, \n       seed = seed)\n}"},{"path":"parallelization-training.html","id":"parallelization-training","chapter":"8 Parallelization training","heading":"8 Parallelization training","text":"Models L2 Regularized Logistic Regression run fast compared others Random Forest Regression. Parallelization can help speeding model training.future-map() function furrr package help speed training.applies function element vector via futures.future_map() returns list.Important: Add future specific options use workers. must result call furrr_options(), eg. options = furrr_options(seed=TRUE)","code":""},{"path":"parallelization-training.html","id":"parallel-iterative-model-training","chapter":"8 Parallelization training","heading":"8.1 Parallel iterative model training","text":"","code":""},{"path":"parallelization-training.html","id":"combine-performance-results","chapter":"8 Parallelization training","heading":"8.2 Combine performance results","text":"","code":"\nperformance <- readRDS(\"data/model_training_results.rds\") %>%\n  map(pluck, \"trained_model\") %>%\n  combine_hp_performance()\n\n  saveRDS(performance, \"data/performance.rds\")\n  \n  resave(performance, file = \"data/model_training_results.rda\")"},{"path":"exploring-model-performance.html","id":"exploring-model-performance","chapter":"9 Exploring model performance","heading":"9 Exploring model performance","text":"Combined results contains:Data (dat): 3-column dataframe including:Alpha (\\(\\alpha\\): parameter determines weighting used. ridge regression = 0, Lasso = 1, elastic regression 0 1.Lambda (\\(\\lambda\\)): Regularization Parameter, constant fine-tune amount penalty. Good value \\(\\lambda\\) critical.AUC: aggregated metric evaluates well logistic regression model classifies positive negative outcomes possible cutoffs.Params (params): LambdaMetric (metric): AUC","code":""},{"path":"exploring-model-performance.html","id":"performance-results","chapter":"9 Exploring model performance","heading":"9.0.1 Performance results","text":"","code":"\n readRDS(\"data/performance.rds\")$dat %>% \n  tail()   alpha lambda       AUC\n40   0.5    0.1 0.5640272\n41   0.5    1.0 0.5000000\n42   0.5    2.0 0.5000000\n43   1.0    0.1 0.5190963\n44   1.0    1.0 0.5000000\n45   1.0    2.0 0.5000000"},{"path":"exploring-model-performance.html","id":"performance-plot","chapter":"9 Exploring model performance","heading":"9.0.2 Performance plot","text":"InterpretationThe dot represents metric mean (mean_AUC).bar represents + - standard deviation.","code":"\nplot_hp_performance(readRDS(\"data/performance.rds\")$dat, lambda, AUC)"},{"path":"exploring-model-performance.html","id":"top-three-auc-values","chapter":"9 Exploring model performance","heading":"9.1 Top three AUC values","text":"","code":"\nreadRDS(\"data/performance.rds\")$dat %>%\n  group_by(alpha, lambda) %>%\n  summarise(mean_AUC = mean(AUC), \n            lquartile = quantile(AUC, prob=0.25),\n            uquartile = quantile(AUC, prob=0.75),\n            .groups=\"drop\") %>%\n  top_n(n=3, mean_AUC)  %>% \n  ggplot(aes(x = lambda, y = mean_AUC, color = as.character(alpha))) +\n  geom_line() +\n  geom_point(color = \"black\")"},{"path":"exploring-model-performance.html","id":"using-helper-function","chapter":"9 Exploring model performance","heading":"9.2 Using helper function","text":"know different hyperparameter options dafault values different modeling approaches","code":"\nget_hyperparams_list(hyper_genus_preprocess, \"glmnet\")$lambda\n[1] 1e-04 1e-03 1e-02 1e-01 1e+00 1e+01\n\n$alpha\n[1] 0\nget_hyperparams_list(hyper_genus_preprocess, \"rf\")$mtry\n[1]  4  7 14\nget_hyperparams_list(hyper_genus_preprocess, \"svmRadial\")$C\n[1] 1e-03 1e-02 1e-01 1e+00 1e+01 1e+02\n\n$sigma\n[1] 1e-06 1e-05 1e-04 1e-03 1e-02 1e-01\nget_hyperparams_list(hyper_genus_preprocess, \"rpart2\") # Decision tree$maxdepth\n[1]  1  2  4  8 16 30\nget_hyperparams_list(hyper_genus_preprocess, \"xgbTree\") # Xboost$nrounds\n[1] 100\n\n$gamma\n[1] 0\n\n$eta\n[1] 0.001 0.010 0.100 1.000\n\n$max_depth\n[1]  1  2  4  8 16 30\n\n$colsample_bytree\n[1] 0.8\n\n$min_child_weight\n[1] 1\n\n$subsample\n[1] 0.4 0.5 0.6 0.7"},{"path":"imap-github-repos.html","id":"imap-github-repos","chapter":"A IMAP GitHub Repos","heading":"A IMAP GitHub Repos","text":"","code":""},{"path":"machine-learning-rulegraph.html","id":"machine-learning-rulegraph","chapter":"B Machine Learning Rulegraph","heading":"B Machine Learning Rulegraph","text":"","code":""},{"path":"session-information.html","id":"session-information","chapter":"C Session Information","heading":"C Session Information","text":"Reproducibility relies ability precisely recreate working environment, session information serves vital reference achieve consistency. record details R environment, package versions, system settings computing environment time analysis.","code":"\nlibrary(sessioninfo)\n\n# Get session info\ninfo <- capture.output(print(session_info()))\n\n# Define patterns to exclude\nexclude_patterns <- c(\"/Users/.*\", \"Africa/Dar_es_Salaam\") # This line is location-dependent\n\n# Exclude lines containing specific information\ninfo_filtered <- info[!grepl(paste(exclude_patterns, collapse = \"|\"), info)]\n\n# Save the filtered session info to a text file in the root directory without line numbers\ncat(info_filtered, file = \"session_info.txt\", sep = \"\\n\")"},{"path":"references.html","id":"references","chapter":"References","heading":"References","text":"","code":""}]
